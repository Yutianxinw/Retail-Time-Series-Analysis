{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":18599,"databundleVersionId":1236839,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-04T02:28:01.595594Z","iopub.execute_input":"2025-05-04T02:28:01.595981Z","iopub.status.idle":"2025-05-04T02:28:01.601915Z","shell.execute_reply.started":"2025-05-04T02:28:01.595957Z","shell.execute_reply":"2025-05-04T02:28:01.601220Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/m5-forecasting-accuracy/calendar.csv\n/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\n/kaggle/input/m5-forecasting-accuracy/sell_prices.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\n/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Forecasting and Submission (Simplified and Improved Baseline using 3-layer GRU) ===\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nfrom keras.models import Sequential\nfrom keras.layers import GRU, Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom sklearn.preprocessing import MinMaxScaler\n\n# === Load sales data ===\nsales = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sales_train_validation.csv\")\nsales_data = sales.iloc[:, 6:].T\nsales_data.columns = sales['id'].values\n\n# === Load and preprocess calendar features ===\ncalendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\ncalendar_cols = ['wday', 'month', 'year', 'event_name_1', 'event_name_2', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI']\ncalendar_train = calendar.iloc[:1913][calendar_cols].copy().fillna(\"none\")\ncalendar_feats = pd.get_dummies(calendar_train, columns=['event_name_1', 'event_name_2', 'event_type_1'])\n\nwith open(\"calendar_columns.pkl\", \"wb\") as f:\n    pickle.dump(calendar_feats.columns.tolist(), f)\n\ncombined = pd.concat([sales_data.reset_index(drop=True), calendar_feats], axis=1)\n\n# === Scale data ===\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(combined)\n\n# === Prepare training sequences ===\ninput_window = 28\nnum_items = 30490\nX_train, y_train = [], []\nfor i in range(input_window, scaled_data.shape[0]):\n    X_train.append(scaled_data[i - input_window:i])\n    y_train.append(scaled_data[i][:num_items])\nX_train = np.array(X_train, dtype=np.float32)\ny_train = np.array(y_train, dtype=np.float32)\n\n# === Load calendar for inference ===\nwith open(\"calendar_columns.pkl\", \"rb\") as f:\n    calendar_columns = pickle.load(f)\ncalendar = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/calendar.csv\")\ncalendar_future_val = pd.get_dummies(calendar.iloc[1913:1941][calendar_cols].fillna(\"none\"), columns=['event_name_1', 'event_name_2', 'event_type_1'])\ncalendar_future_val = calendar_future_val.reindex(columns=calendar_columns, fill_value=0)\ncalendar_next_28_val = calendar_future_val.values.astype(np.float32)\n\n# === Define 3-layer GRU Model Architecture ===\ndef build_3layer_gru_model(input_timesteps, num_features, num_items, dropout_rate=0.2):\n    model = Sequential()\n    model.add(GRU(64, return_sequences=True, input_shape=(input_timesteps, num_features), dropout=dropout_rate, recurrent_dropout=0.1))\n    model.add(GRU(128, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.1))\n    model.add(GRU(128, return_sequences=False, dropout=dropout_rate, recurrent_dropout=0.1))\n    model.add(Dense(num_items))\n    optimizer = Adam(learning_rate=0.001)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    return model, [lr_callback, early_stop]\n\n# === Train Model ===\nmodel, callbacks = build_3layer_gru_model(28, X_train.shape[2], 30490)\nmodel.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, callbacks=callbacks, verbose=1)\n\n# === Forecasting Function ===\ndef forecast_28_days(model, input_sequence, calendar_future, num_items):\n    window = input_sequence.shape[0]\n    input_data = input_sequence.copy()\n    predictions = []\n    for day in range(28):\n        x_input = input_data[-window:].reshape(1, window, -1)\n        y_pred = model.predict(x_input, verbose=0)\n        full_pred_row = np.concatenate([y_pred[0], np.zeros(calendar_future.shape[1])]).reshape(1, -1)\n        y_pred_original = scaler.inverse_transform(full_pred_row)[:, :num_items]\n        next_input_row = np.concatenate([y_pred[0], calendar_future[day]])\n        input_data = np.vstack([input_data, next_input_row])\n        predictions.append(y_pred_original)\n    return np.vstack(predictions)\n\n# === Forecast and Save ===\ndef make_submission(calendar_range, ids_replace_str, output_file):\n    calendar_future = pd.get_dummies(calendar.iloc[calendar_range][calendar_cols].fillna(\"none\"), columns=['event_name_1', 'event_name_2', 'event_type_1'])\n    calendar_future = calendar_future.reindex(columns=calendar_columns, fill_value=0)\n    calendar_next_28 = calendar_future.values.astype(np.float32)\n    last_28_days_input = scaled_data[-28:, :].astype(np.float32)\n    forecast = forecast_28_days(model, last_28_days_input, calendar_next_28, num_items=30490)\n    forecast = np.clip(forecast, 0, None).T\n    ids = [i.replace(\"validation\", ids_replace_str) for i in sales['id'].values]\n    submission_df = pd.DataFrame(forecast, columns=[f\"F{i}\" for i in range(1, 29)])\n    submission_df.insert(0, \"id\", ids)\n    submission_df.to_csv(output_file, index=False)\n    print(f\"✅ {output_file} saved successfully!\")\n\nmake_submission(slice(1913, 1941), \"validation\", \"submission_val.csv\")\nmake_submission(slice(1941, 1969), \"evaluation\", \"submission_eval.csv\")\n\n# === Combine and Save Final Submission ===\nsubmission_val = pd.read_csv(\"submission_val.csv\")\nsubmission_eval = pd.read_csv(\"submission_eval.csv\")\nsubmission_all = pd.concat([submission_val, submission_eval], axis=0).reset_index(drop=True)\nsample = pd.read_csv(\"/kaggle/input/m5-forecasting-accuracy/sample_submission.csv\")\nfinal_submission = sample[['id']].merge(submission_all, on='id', how='left')\nassert final_submission['id'].duplicated().sum() == 0\nassert final_submission.isnull().sum().sum() == 0\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nprint(f\"✅ Final submission saved: {len(final_submission)} rows → final_submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-04T02:28:04.955395Z","iopub.execute_input":"2025-05-04T02:28:04.956095Z","iopub.status.idle":"2025-05-04T02:32:31.878699Z","shell.execute_reply.started":"2025-05-04T02:28:04.956069Z","shell.execute_reply":"2025-05-04T02:32:31.877911Z"}},"outputs":[{"name":"stderr","text":"2025-05-04 02:28:06.448621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746325686.657155      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746325686.718786      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nI0000 00:00:1746325712.574219      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746325712.574849      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 182ms/step - loss: 0.0168 - val_loss: 0.0192 - learning_rate: 0.0010\nEpoch 2/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - loss: 0.0132 - val_loss: 0.0186 - learning_rate: 0.0010\nEpoch 3/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0126 - val_loss: 0.0185 - learning_rate: 0.0010\nEpoch 4/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0122 - val_loss: 0.0186 - learning_rate: 0.0010\nEpoch 5/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0121 - val_loss: 0.0185 - learning_rate: 0.0010\nEpoch 6/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 120ms/step - loss: 0.0120 - val_loss: 0.0184 - learning_rate: 0.0010\nEpoch 7/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - loss: 0.0118 - val_loss: 0.0185 - learning_rate: 0.0010\nEpoch 8/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 124ms/step - loss: 0.0118 - val_loss: 0.0183 - learning_rate: 0.0010\nEpoch 9/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0117 - val_loss: 0.0182 - learning_rate: 0.0010\nEpoch 10/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0117 - val_loss: 0.0184 - learning_rate: 0.0010\nEpoch 11/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 118ms/step - loss: 0.0117 - val_loss: 0.0183 - learning_rate: 0.0010\nEpoch 12/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0116\nEpoch 12: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - loss: 0.0116 - val_loss: 0.0182 - learning_rate: 0.0010\nEpoch 13/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0115 - val_loss: 0.0182 - learning_rate: 5.0000e-04\nEpoch 14/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 120ms/step - loss: 0.0114 - val_loss: 0.0182 - learning_rate: 5.0000e-04\nEpoch 15/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0114\nEpoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0114 - val_loss: 0.0183 - learning_rate: 5.0000e-04\nEpoch 16/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 128ms/step - loss: 0.0112 - val_loss: 0.0182 - learning_rate: 2.5000e-04\nEpoch 17/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0112 - val_loss: 0.0182 - learning_rate: 2.5000e-04\nEpoch 18/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 0.0113\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 125ms/step - loss: 0.0113 - val_loss: 0.0182 - learning_rate: 2.5000e-04\nEpoch 19/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0112 - val_loss: 0.0182 - learning_rate: 1.2500e-04\nEpoch 20/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 123ms/step - loss: 0.0113 - val_loss: 0.0182 - learning_rate: 1.2500e-04\nEpoch 21/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - loss: 0.0111\nEpoch 21: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 1.2500e-04\nEpoch 22/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 6.2500e-05\nEpoch 23/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 6.2500e-05\nEpoch 24/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 0.0112\nEpoch 24: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0112 - val_loss: 0.0182 - learning_rate: 6.2500e-05\nEpoch 25/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 124ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 3.1250e-05\nEpoch 26/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 126ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 3.1250e-05\nEpoch 27/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: 0.0113\nEpoch 27: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 117ms/step - loss: 0.0113 - val_loss: 0.0182 - learning_rate: 3.1250e-05\nEpoch 28/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 123ms/step - loss: 0.0110 - val_loss: 0.0182 - learning_rate: 1.5625e-05\nEpoch 29/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 122ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 1.5625e-05\nEpoch 30/30\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - loss: 0.0111\nEpoch 30: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 121ms/step - loss: 0.0111 - val_loss: 0.0182 - learning_rate: 1.5625e-05\n✅ submission_val.csv saved successfully!\n✅ submission_eval.csv saved successfully!\n✅ Final submission saved: 60980 rows → final_submission.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}